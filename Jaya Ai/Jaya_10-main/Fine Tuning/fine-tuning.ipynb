{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55eda15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install transformers peft bitsandbytes accelerate --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efb685a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, torch\n",
    "\n",
    "DTYPE_BYTES = {\n",
    "    'fp32': 4,\n",
    "    'fp16': 2,\n",
    "    'bf16': 2,\n",
    "    'int8': 1,\n",
    "    'int4': 0.5,\n",
    "}\n",
    "\n",
    "def vram_estimate(num_params: int,\n",
    "                  base_dtype: str = 'fp16',\n",
    "                  trainable_ratio: float = 1.0,\n",
    "                  train_dtype: str = 'fp16',\n",
    "                  optimizer: str = 'adam'):\n",
    "    \"\"\"Return VRAM (MB) for weights + grads + optimiser.\"\"\"\n",
    "    base_mem = num_params * DTYPE_BYTES[base_dtype]\n",
    "    train_params = num_params * trainable_ratio\n",
    "    grad_mem = train_params * DTYPE_BYTES[train_dtype]\n",
    "    opt_mult = 2 if optimizer == 'adam' else 0  # momentum+variance\n",
    "    opt_mem = train_params * DTYPE_BYTES[train_dtype] * opt_mult\n",
    "    total = (base_mem + grad_mem + opt_mem) / (1024 ** 2)\n",
    "    return dict(weights_MB=base_mem / (1024**2),\n",
    "                grads_MB=grad_mem / (1024**2),\n",
    "                optim_MB=opt_mem / (1024**2),\n",
    "                total_MB=total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "388ade4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full fine‑tune   : {'weights_MB': '12,856 MB', 'grads_MB': '12,856 MB', 'optim_MB': '25,711 MB', 'total_MB': '51,422 MB'}\n",
      "LoRA (1% params) : {'weights_MB': '12,856 MB', 'grads_MB': '129 MB', 'optim_MB': '257 MB', 'total_MB': '13,241 MB'}\n",
      "QLoRA (int4 base): {'weights_MB': '3,214 MB', 'grads_MB': '129 MB', 'optim_MB': '257 MB', 'total_MB': '3,600 MB'}\n"
     ]
    }
   ],
   "source": [
    "llama_7b = 6.74e9  # params\n",
    "\n",
    "def pretty(d):\n",
    "    return {k: f\"{v:,.0f} MB\" for k, v in d.items()}\n",
    "\n",
    "full_ft   = vram_estimate(llama_7b, base_dtype='fp16', trainable_ratio=1.0)\n",
    "lora_1pct = vram_estimate(llama_7b, base_dtype='fp16', trainable_ratio=0.01)\n",
    "qlora     = vram_estimate(llama_7b, base_dtype='int4', trainable_ratio=0.01, train_dtype='fp16')\n",
    "\n",
    "print('Full fine‑tune   :', pretty(full_ft))\n",
    "print('LoRA (1% params) :', pretty(lora_1pct))\n",
    "print('QLoRA (int4 base):', pretty(qlora))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c3d7ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weights_MB': '6,199 MB', 'grads_MB': '62 MB', 'optim_MB': '124 MB', 'total_MB': '6,385 MB'}\n"
     ]
    }
   ],
   "source": [
    "# tweak me ↑\n",
    "model_params   = 13e9      # e.g., 13B model\n",
    "trainable_ratio= 0.0025    # 0.25% (r=8 LoRA) \n",
    "base_dtype     = 'int4'    # 'fp16', 'int8', 'int4'\n",
    "train_dtype    = 'fp16'\n",
    "print(pretty(vram_estimate(model_params, base_dtype, trainable_ratio, train_dtype=train_dtype)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6103652",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Shashank\\Swecha\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers and GPU quantization are unavailable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model PeftModel(\n",
      "  (base_model): LoraModel(\n",
      "    (model): GPT2LMHeadModel(\n",
      "      (transformer): GPT2Model(\n",
      "        (wte): Embedding(50257, 2)\n",
      "        (wpe): Embedding(1024, 2)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "        (h): ModuleList(\n",
      "          (0-1): 2 x GPT2Block(\n",
      "            (ln_1): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): GPT2Attention(\n",
      "              (c_attn): lora.Linear(\n",
      "                (base_layer): Conv1D(nf=6, nx=2)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=6, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (c_proj): Conv1D(nf=2, nx=2)\n",
      "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (ln_2): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): GPT2MLP(\n",
      "              (c_fc): Conv1D(nf=8, nx=2)\n",
      "              (c_proj): Conv1D(nf=2, nx=8)\n",
      "              (act): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (ln_f): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (lm_head): Linear(in_features=2, out_features=50257, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Base model params   : 102,714\n",
      "LoRA trainable params: 128  (0.12%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Shashank\\Swecha\\.venv\\Lib\\site-packages\\peft\\tuners\\lora\\layer.py:1803: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained('sshleifer/tiny-gpt2')\n",
    "base_params = sum(p.numel() for p in base.parameters())\n",
    "\n",
    "lora_cfg = LoraConfig(r=8, target_modules=['c_attn'])\n",
    "lora_model = get_peft_model(base, lora_cfg)\n",
    "\n",
    "print(\"Model\", lora_model)\n",
    "\n",
    "train_params = sum(p.numel() for p in lora_model.parameters() if p.requires_grad)\n",
    "print(f'Base model params   : {base_params:,}')\n",
    "print(f'LoRA trainable params: {train_params:,}  ({train_params/base_params:.2%})')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
